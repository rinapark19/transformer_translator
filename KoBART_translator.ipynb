{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import BartTokenizer\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "  def __init__(self, df, tokenizer, max_len, ignore_index=-100, verbose=True):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.df = df\n",
    "    self.len = len(self.df)\n",
    "    self.pad_index = self.tokenizer.pad_token_id\n",
    "    self.ignore_index = ignore_index\n",
    "\n",
    "  def add_padding_data(self, inputs):\n",
    "    if len(inputs) < self.max_len:\n",
    "      pad = np.array([self.pad_index] * (self.max_len - len(inputs)))\n",
    "      inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "      inputs = inputs[:self.max_len]\n",
    "    return inputs\n",
    "\n",
    "  def add_ignored_data(self, inputs):\n",
    "    if len(inputs) < self.max_len:\n",
    "      pad = np.array([self.ignore_index] * (self.max_len - len(inputs)))\n",
    "      inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "      inputs = inputs[:self.max_len]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "  def __getitem__(self, idx, verbose=True):\n",
    "    instance = self.df.iloc[idx]\n",
    "    input_ids = self.tokenizer.encode(instance['원문'])\n",
    "    input_ids = np.append(input_ids, self.tokenizer.eos_token_id)\n",
    "    input_ids = self.add_padding_data(input_ids)\n",
    "    input_ids = np.insert(input_ids, 0, self.tokenizer.bos_token_id)\n",
    "\n",
    "    label_ids = self.tokenizer.encode(instance['번역문'])\n",
    "    label_ids.append(self.tokenizer.eos_token_id)\n",
    "    label_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "\n",
    "    dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "    dec_input_ids += label_ids[:-1]\n",
    "    dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "    label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "    input_ids = torch.tensor(np.array(input_ids)).long()\n",
    "    decoder_input_ids = torch.tensor(np.array(dec_input_ids)).long()\n",
    "\n",
    "    attention_mask = input_ids.ne(self.tokenizer.pad_token_id).float()\n",
    "\n",
    "    return {'input_ids': input_ids,\n",
    "            #'attention_mask': input_ids.ne(self.tokenizer.pad_token_id).float(),\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            # 'decoder_attention_mask': decoder_input_ids.ne(self.tokenizer.pad_token_id).float(),\n",
    "            'labels': np.array(label_ids, dtype = np.int_)}\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  preds, labels = pred\n",
    "\n",
    "  preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "  print(\"원문: \", val['원문'][0])\n",
    "  print(\"번역 정답\", labels[0])\n",
    "  print(\"번역 결과: \", preds[0])\n",
    "\n",
    "  reference = preds[0].split()\n",
    "  candidate = []\n",
    "  candidate.append(labels[0].split())\n",
    "  bleu = sentence_bleu(references = candidate, hypothesis=reference, weights=(1, 0, 0, 0))\n",
    "  return {\"BLEU score\": bleu }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "stop = 3\n",
    "epoch = 10\n",
    "batch = 4\n",
    "seed = 42\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"english_korean_data/train_small.csv\", encoding=\"cp949\")\n",
    "val = pd.read_csv(\"english_korean_data/test_open.csv\", encoding=\"cp949\")\n",
    "train_dataset = TranslationDataset(train, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 256)\n",
    "val_dataset = TranslationDataset(val, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id = tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "config = BartConfig.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "config.encoder_embed_dim = 768  \n",
    "config.encoder_embed_path = None\n",
    "\n",
    "\n",
    "encoder_embedding = torch.nn.Embedding(config.vocab_size, config.encoder_embed_dim)\n",
    "\n",
    "\n",
    "original_model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "original_model.model.encoder.embed_tokens = encoder_embedding\n",
    "\n",
    "\n",
    "model = original_model\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
    "                                                        num_warmup_steps = 100,\n",
    "                                                        num_training_steps = epoch * len(train_dataset) * batch,\n",
    "                                                        last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(run_name = \"KoBART_translator\",\n",
    "                                output_dir = \"./BART_translator_2\",\n",
    "                                evaluation_strategy=\"steps\",\n",
    "                                eval_steps = 100,\n",
    "                                save_steps = 100,\n",
    "                                save_total_limit=2,\n",
    "\n",
    "                                per_device_train_batch_size= batch,\n",
    "                                per_device_eval_batch_size = batch,\n",
    "                                gradient_accumulation_steps = 16,\n",
    "                                num_train_epochs = epoch,\n",
    "\n",
    "                                load_best_model_at_end = True,\n",
    "                                #fp16=True,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                predict_with_generate=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model = model,\n",
    "                        tokenizer = tokenizer,\n",
    "                        args = args,\n",
    "                        train_dataset = train_dataset,\n",
    "                        eval_dataset = val_dataset,\n",
    "                        compute_metrics = compute_metrics,\n",
    "                        optimizers = (optimizer, lr_scheduler),\n",
    "                        data_collator = collator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1560 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                  \n",
      "  6%|▋         | 100/1560 [04:22<25:15,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih there are are are are\n",
      "{'eval_loss': 2.2680821418762207, 'eval_BLEU score': 0, 'eval_runtime': 158.5952, 'eval_samples_per_second': 63.054, 'eval_steps_per_second': 15.763, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 200/1560 [06:08<23:37,  1.04s/it]   /home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                  \n",
      " 13%|█▎        | 200/1560 [08:47<23:37,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih it is an ordered it is\n",
      "{'eval_loss': 2.127504348754883, 'eval_BLEU score': 0.06993452279385044, 'eval_runtime': 158.8315, 'eval_samples_per_second': 62.96, 'eval_steps_per_second': 15.74, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 19%|█▉        | 300/1560 [13:10<21:51,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih there are only only \n",
      "{'eval_loss': 2.0682413578033447, 'eval_BLEU score': 0, 'eval_runtime': 158.2308, 'eval_samples_per_second': 63.199, 'eval_steps_per_second': 15.8, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 26%|██▌       | 400/1560 [17:32<20:05,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih it's a lot of color, s\n",
      "{'eval_loss': 2.0137991905212402, 'eval_BLEU score': 0, 'eval_runtime': 157.2427, 'eval_samples_per_second': 63.596, 'eval_steps_per_second': 15.899, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 500/1560 [19:17<18:26,  1.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4676, 'learning_rate': 2.9999925941003032e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 32%|███▏      | 500/1560 [21:55<18:26,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih it's a refund on the ref\n",
      "{'eval_loss': 1.9806348085403442, 'eval_BLEU score': 0, 'eval_runtime': 157.4413, 'eval_samples_per_second': 63.516, 'eval_steps_per_second': 15.879, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 38%|███▊      | 600/1560 [26:18<16:41,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih it's a refund on the ref\n",
      "{'eval_loss': 2.185455083847046, 'eval_BLEU score': 0, 'eval_runtime': 158.183, 'eval_samples_per_second': 63.218, 'eval_steps_per_second': 15.804, 'epoch': 3.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 45%|████▍     | 700/1560 [30:42<14:56,  1.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we have to get a refund on\n",
      "{'eval_loss': 1.9353324174880981, 'eval_BLEU score': 0, 'eval_runtime': 157.888, 'eval_samples_per_second': 63.336, 'eval_steps_per_second': 15.834, 'epoch': 4.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 51%|█████▏    | 800/1560 [35:06<13:38,  1.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we get a refund on the stor\n",
      "{'eval_loss': 1.927201747894287, 'eval_BLEU score': 0, 'eval_runtime': 157.623, 'eval_samples_per_second': 63.443, 'eval_steps_per_second': 15.861, 'epoch': 5.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 58%|█████▊    | 900/1560 [39:33<11:42,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih it's a bit because I'm\n",
      "{'eval_loss': 1.917406678199768, 'eval_BLEU score': 0, 'eval_runtime': 158.6617, 'eval_samples_per_second': 63.027, 'eval_steps_per_second': 15.757, 'epoch': 5.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1000/1560 [41:21<10:02,  1.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.867, 'learning_rate': 2.9999625077581203e-05, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 64%|██████▍   | 1000/1560 [43:59<10:02,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we can't be able to get \n",
      "{'eval_loss': 1.9005603790283203, 'eval_BLEU score': 0, 'eval_runtime': 158.0035, 'eval_samples_per_second': 63.29, 'eval_steps_per_second': 15.822, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 71%|███████   | 1100/1560 [48:25<08:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we get a lot of color, s\n",
      "{'eval_loss': 1.9036136865615845, 'eval_BLEU score': 0, 'eval_runtime': 157.9129, 'eval_samples_per_second': 63.326, 'eval_steps_per_second': 15.832, 'epoch': 7.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1200/1560 [50:14<06:17,  1.05s/it]  /home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                   \n",
      " 77%|███████▋  | 1200/1560 [52:51<06:17,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we go to the company set to\n",
      "{'eval_loss': 1.886002540588379, 'eval_BLEU score': 0.07581633246407919, 'eval_runtime': 157.1868, 'eval_samples_per_second': 63.619, 'eval_steps_per_second': 15.905, 'epoch': 7.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1300/1560 [54:36<04:29,  1.04s/it]  /home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                   \n",
      " 83%|████████▎ | 1300/1560 [57:13<04:29,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Ih we go to the company set \n",
      "{'eval_loss': 1.904558777809143, 'eval_BLEU score': 0.06993452279385044, 'eval_runtime': 157.3752, 'eval_samples_per_second': 63.542, 'eval_steps_per_second': 15.886, 'epoch': 8.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1400/1560 [58:58<02:46,  1.04s/it]  /home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                   \n",
      " 90%|████████▉ | 1400/1560 [1:01:36<02:46,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Then I will give you a refund \n",
      "{'eval_loss': 1.8662301301956177, 'eval_BLEU score': 0.13986904558770089, 'eval_runtime': 157.7864, 'eval_samples_per_second': 63.377, 'eval_steps_per_second': 15.844, 'epoch': 8.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1500/1560 [1:03:22<01:02,  1.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7048, 'learning_rate': 2.9999092785685634e-05, 'epoch': 9.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/scar/Desktop/chaerin/translator_LLM/.env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                     \n",
      " 96%|█████████▌| 1500/1560 [1:06:00<01:02,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:  너희 아빠랑 이번 주말에 보러 다녀와야겠네 그럼.\n",
      "번역 정답 Then you should go and watch it with your dad this weekend.\n",
      "번역 결과:  Then I will get an actual if\n",
      "{'eval_loss': 1.866457223892212, 'eval_BLEU score': 0.06993452279385044, 'eval_runtime': 158.0103, 'eval_samples_per_second': 63.287, 'eval_steps_per_second': 15.822, 'epoch': 9.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [1:07:04<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4024.6553, 'train_samples_per_second': 24.847, 'train_steps_per_second': 0.388, 'train_loss': 1.99858518747183, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1560, training_loss=1.99858518747183, metrics={'train_runtime': 4024.6553, 'train_samples_per_second': 24.847, 'train_steps_per_second': 0.388, 'train_loss': 1.99858518747183, 'epoch': 9.98})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,  # 현재 학습 epoch\n",
    "    'model_state_dict': model.state_dict(),  # 모델 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
    "}, 'translator3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text, label):\n",
    "  tmp = [tokenizer.bos_token_id] + tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "  out = model.generate(input_ids = torch.tensor(tmp)[None, :].to(device))\n",
    "  result = tokenizer.decode(out[0])\n",
    "\n",
    "  print(\"번역 결과: \", result)\n",
    "\n",
    "  reference = result.split()\n",
    "  candidate = []\n",
    "  candidate.append(label.split())\n",
    "  bleu = sentence_bleu(references=candidate, hypothesis=reference, weights=(1, 0, 0, 0))\n",
    "\n",
    "  print(\"BLEU score\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 결과:  </s><s> here is looks, are on</s>\n",
      "BLEU score 0\n"
     ]
    }
   ],
   "source": [
    "infer(\"아쉽지만 그러면 한 명은 기다려야 할 것 같네요.\", \"This room stinks of cigarette smells. I want to change rooms.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
